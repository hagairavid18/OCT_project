image : 0
torch.Size([1, 3, 496, 512])
1
torch.Size([3, 496, 512])
Traceback (most recent call last):
  File "run_layer_cam.py", line 103, in <module>
    cam = layer_cam.generate_cam(images[0], labels.item())
  File "/home/labs/testing/class57/OCTransformer/breakdown/testings/vit_att_trial/layer_cam.py", line 72, in generate_cam
    conv_output, model_output = self.extractor.forward_pass(input_image)
  File "/home/labs/testing/class57/OCTransformer/breakdown/testings/vit_att_trial/layer_cam.py", line 49, in forward_pass
    conv_output, x = self.forward_pass_on_convolutions(x)
  File "/home/labs/testing/class57/OCTransformer/breakdown/testings/vit_att_trial/layer_cam.py", line 35, in forward_pass_on_convolutions
    x = module(x)  # Forward
  File "/home/labs/testing/class57/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/labs/testing/class57/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 446, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/labs/testing/class57/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 442, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Expected 4-dimensional input for 4-dimensional weight [64, 3, 7, 7], but got 3-dimensional input of size [3, 496, 512] instead